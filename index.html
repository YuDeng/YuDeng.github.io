<!DOCTYPE HTML>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta name="author" content="Yu Deng">
  <meta name="description" content="Yu Deng's Homepage">
  <meta name="keywords" content="Yu Deng,邓誉,homepage,主页,PhD,computer vision,MSRA,Tsinghua,3D face,3D reconstruction,image generation,implicit field">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Yu Deng (邓誉)'s Homepage</title>
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yu Deng (邓誉)</name>
              </p>
              <p style="text-align:center">
                Email: t-yudeng[at]microsoft.com &nbsp; &nbsp;&nbsp;&nbsp; <a href="https://scholar.google.com/citations?user=N3F3H0sAAAAJ&hl=en">Google Scholar</a> &nbsp; &nbsp;&nbsp;&nbsp;<a href="https://github.com/YuDeng">Github</a>
              </p>
              <p>I am a Joint PhD student of <a href="http://www.castu.tsinghua.edu.cn/">Institute For Advanced Study in Tsinghua University</a> and <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia(MSRA)</a>,
		under the supervision of <a href="https://www.microsoft.com/en-us/research/people/hshum/">Prof. Harry Shum</a>. 
                I am currently working as a research intern in <a href="https://www.microsoft.com/en-us/research/group/visual-computing/">Visual Computing Group</a> in MSRA, mentored by 
		Senior Researcher <a href="http://jlyang.org/">Jiaolong Yang</a> and Partner Research Manager <a href="https://www.microsoft.com/en-us/research/people/xtong/">Xin Tong</a>.
              </p>
              <p>
                I received B.S. from Department of Physics in Tsinghua University, 2017.
              </p>
            </td>
            <td style="padding:15% 7% 7% 7%;width:40%;max-width:40%">
              <a href="images/yu.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/yu.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/gram.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>GRAM: Generative Radiance Manifolds for 3D-Aware Image Generation</papertitle>
                <br>
                <strong>Yu Deng</strong>, Jiaolong Yang, Jianfeng Xiang, Xin Tong
                <br>
                <em>2022 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2022,
                <br>
                <a href="https://arxiv.org/abs/2112.08867">[PDF]</a>
                <a href="https://yudeng.github.io/GRAM/">[Project]</a>
				<a href="images/gram.txt">[BibTeX]</a>
                <br>
                <p>We propose Generative Radiance Manifolds (GRAM), a method that can generate 3D-consistent images with explicit camera control, trained on only unstructured 2D images.</p>
            </td>
        </tr>

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                <img src='images/arxiv_difnet.png' style="width:100%;max-width:100%; position: absolute;top: -5%">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Deformed Implicit Field: Modeling 3D Shapes with Learned Dense Correspondence</papertitle>
                <br>
                <strong>Yu Deng</strong>, Jiaolong Yang, Xin Tong
                <br>
                <em>2021 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2021,
                <br>
                <a href="https://arxiv.org/abs/2011.13650">[PDF]</a>
                <a href="https://github.com/microsoft/DIF-Net">[Code]</a>
				<a href="images/dif_net.txt">[BibTeX]</a>
                <br>
                <p>We propose a novel Deformed Implicit Field (DIF) representation for modeling 3D shapes of a category and generating dense correspondences among shapes with structure variations.</p>
            </td>
        </tr>
          
        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one" >
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/disco.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Disentangled and Controllable Face Image Generation via 3D Imitative-Contrastive Learning</papertitle>
                <br>
                <strong>Yu Deng</strong>, Jiaolong Yang, Dong Chen, Fang Wen, Xin Tong
                <br>
                <em>2020 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2020, 
				<strong><font color="#FF0000">Oral Presentation</font></strong>
                <br>
                <a href="https://arxiv.org/abs/2004.11660">[PDF]</a>
                <a href="https://github.com/microsoft/DiscoFaceGAN">[Code]</a>
				<a href="images/discoface.txt">[BibTeX]</a>
                <br>
                <p>We propose DiscoFaceGAN, an approach for face image generation of virtual people with disentangled, precisely-controllable latent representations for identity, expression, pose, and illumination. </p>
            </td>
        </tr>

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one" >
                    <img src='images/cvpr20_deep3dportrait.png' style="width:100%;max-width:100%; position: absolute;top: -5%">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Deep 3D Portrait from a Single Image</papertitle>
                <br>
                Sicheng Xu, Jiaolong Yang, Dong Chen, Fang Wen, <strong>Yu Deng</strong>, Yunde Jia, Xin Tong
                <br>
                <em>2020 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2020, 
                <br>
                <a href="https://arxiv.org/abs/2004.11598">[PDF]</a>
                <a href="https://github.com/sicxu/Deep3dPortrait">[Code]</a>
				<a href="images/deep3dportrait.txt">[BibTeX]</a>
                <br>
                <p>We propose a learning-based approach for recovering the 3D geometry of human head from a single portrait image without any ground-truth 3D data.</p>
            </td>
        </tr>

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one" >
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/accurate3d.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set</papertitle>
                <br>
                <strong>Yu Deng</strong>, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde Jia, Xin Tong
                <br>
                <em>2019 IEEE Conference on Computer Vision and Pattern Recognition Workshop on AMFG</em>, CVPRW 2019, 
                <strong><font color="#FF0000">Best Paper Award</font></strong>
                <br>
                <a href="https://arxiv.org/abs/1903.08527">[PDF]</a>
                <a href="https://github.com/microsoft/Deep3DFaceReconstruction">[Code]</a>
				<a href="images/accurate3d.txt">[BibTeX]</a>
                <br>
                <p>We propose a novel deep 3D face reconstruction approach that leverages a robust hybrid loss function and performs multi-image face reconstruction by exploiting complementary information from different images for shape aggregation.</p>
            </td>
        </tr>

        <tr></tr>
            <td style="padding:20px;width:0%;vertical-align:middle">
            </td>
            <td style="padding:20px;width:100%;vertical-align:middle">
		<hr style="margin-top:0px">
                <p>The website template was adapted from <a href="https://panzhang0212.github.io/">Pan Zhang</a>.</p>
            </td>
        </tr>

      </td>
    </tr>
  </table>
</body>

</html>
