<!DOCTYPE HTML>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta name="author" content="Yu Deng">
  <meta name="description" content="Yu Deng's Homepage">
  <meta name="keywords" content="Yu Deng,邓誉,homepage,主页,PhD,computer vision,MSRA,Tsinghua,3D generation,neural rendering, Embodied AI">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Yu Deng (邓誉)'s Homepage</title>
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yu Deng (邓誉)</name>
              </p>
              <p style="text-align:center">
                Email: dengyu[at]microsoft.com &nbsp; &nbsp;&nbsp;&nbsp; <a href="https://scholar.google.com/citations?user=N3F3H0sAAAAJ&hl=en">Google Scholar</a> &nbsp; &nbsp;&nbsp;&nbsp;<a href="https://github.com/YuDeng">Github</a>
              </p>
              <p>I am currently a senior researcher at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia (MSRA)</a>. 
		      My research interests include 3D vision&generation, spatial understanding, and Embodied AI. 
              </p>
              <p>
                Prior to that, I was a researcher at <a href="https://www.linkedin.com/company/xiaobing-ai">Xiaobing.AI</a> working on realistic virtual avatar generation. Before joining Xiaobing, I had received my Ph.D. degree from <a href="https://www.ias.tsinghua.edu.cn/en/">Tsinghua University</a> under the supervision of Prof. <a href="https://www.microsoft.com/en-us/research/people/hshum/">Harry Shum</a> in 2022, and  
                worked closely with Dr. <a href="http://jlyang.org/">Jiaolong Yang</a> and <a href="https://www.microsoft.com/en-us/research/people/xtong/">Xin Tong</a> as a research intern at MSRA from 2017 to 2022.  
	      Before that, I had received my B.S. from Department of Physics in Tsinghua University in 2017.
              </p>
		    
            </td>
            <td style="padding:15% 7% 7% 7%;width:40%;max-width:40%">
              <a href="images/2.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/2.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle" bgcolor="#ffffd0">
                <div class="one">
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/vitra.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle" bgcolor="#ffffd0">
                <papertitle>Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos</papertitle>
                <br>
                Qixiu Li*, <strong>Yu Deng*</strong>, Yaobo Liang*, Lin Luo*, Lei Zhou, Chengtang Yao, Lingqi Zeng, Zhiyuan Feng, Huizhi Liang, Sicheng Xu, Yizhong Zhang, Xi Chen, Hao Chen, Lily Sun, Dong Chen, Jiaolong Yang, Baining Guo (* for equal contribution)
                <br>
                <em>arXiv 2025</em>
                <br>
                <a href="https://arxiv.org/abs/2510.21571">[PDF]</a>
                <a href="https://microsoft.github.io/VITRA/">[Project]</a>
		<a href="https://github.com/microsoft/VITRA/">[Code]</a>
                <br>
                <p>We propose VITRA, a novel approach for pretraining Vision-Language-Action (VLA) models for robotic manipulation using large-scale, unscripted, real-world videos of human hand activities.</p>
            </td>
        </tr>	

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/see.jpg'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes</papertitle>
                <br>
                Zhiyuan Feng*, Zhaolu Kang*, Qijie Wang*, Zhiying Du*, Jiongrui Yan, Shubin Shi, Chengbo Yuan, Huizhi Liang, <strong>Yu Deng<strong>, Qixiu Li, Rushuai Yang, Arctanx An, Leqi Zheng, Weijie Wang, Shawn Chen, Sicheng Xu, Yaobo Liang, Jiaolong Yang, Baining Guo
                <br>
                <em>arXiv 2025</em>
                <br>
                <a href="https://arxiv.org/abs/2510.19400">[PDF]</a>
                <a href="https://github.com/microsoft/MV-RoboBench">[Project]</a>
		<a href="https://github.com/microsoft/MV-RoboBench">[Code]</a>
                <br>
                <p>We propose MV-RoboBench, a benchmark for evaluating the multi-view spatial reasoning capabilities of VLMs in robotic manipulation.</p>
            </td>
        </tr>	

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/moge-2.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>MoGe-2: Accurate Monocular Geometry with Metric Scale and Sharp Details</papertitle>
                <br>
                Ruicheng Wang, Sicheng Xu, Yue Dong, <strong>Yu Deng</strong>, Jianfeng Xiang, Zelong Lv, Guangzhong Sun, Xin Tong, Jiaolong Yang
                <br>
                <em>2025 Conference on Neural Information Processing Systems</em>, NeurIPS 2025
                <br>
                <a href="https://arxiv.org/abs/2507.02546">[PDF]</a>
                <a href="https://wangrc.site/MoGe2Page/">[Project]</a>
		<a href="https://github.com/microsoft/moge">[Code]</a>
                <br>
                <p>We propose MoGe-2, a monocular geometry estimation method with accurate metric-scale prediction and sharp details.</p>
            </td>
        </tr>	

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/vasa3d.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image</papertitle>
                <br>
                Sicheng Xu*, Guojun Chen*, Jiaolong Yang, Yizhong Zhang, <strong>Yu Deng</strong>, Steve Lin, Baining Guo
                <br>
                <em>2025 Conference on Neural Information Processing Systems</em>, NeurIPS 2025
                <br>
                <a href="https://www.microsoft.com/en-us/research/project/vasa-3d/">[PDF]</a>
                <a href="https://www.microsoft.com/en-us/research/project/vasa-3d/">[Project]</a>
                <br>
                <p>We propose VASA-3D, a novel method for creating real-time life-like 3D talking head avatars from a single image.</p>
            </td>
        </tr>	
			
        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle" bgcolor="#ffffd0">
                <div class="one">
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/trellis.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle" bgcolor="#ffffd0">
                <papertitle>Structured 3D Latents for Scalable and Versatile 3D Generation</papertitle>
                <br>
                Jianfeng Xiang, Zelong Lv, Sicheng Xu, <strong>Yu Deng</strong>, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, Jiaolong Yang
                <br>
                <em>2025 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2025,
		    <strong><font color="#FF8080">Spotlight</font></strong>
                <br>
                <a href="https://arxiv.org/abs/2412.01506">[PDF]</a>
                <a href="https://trellis3d.github.io/">[Project]</a>
		<a href="https://github.com/Microsoft/TRELLIS">[Code]</a>
                <br>
                <p>We propose a native 3D generative model built on a unified Structured Latent representation and Rectified Flow Transformers, enabling versatile and high-quality 3D asset creation.</p>
            </td>
        </tr>	

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/cogact.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation</papertitle>
                <br>
                Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, <strong>Yu Deng</strong>, Sicheng Xu, Yizhong Zhang, Xiaofan Wang, Bei Liu, Jianlong Fu, Jianmin Bao, Dong Chen, Yuanchun Shi, Jiaolong Yang, Baining Guo
                <br>
                <em>arXiv 2024</em>,
                <br>
                <a href="https://arxiv.org/abs/2411.19650">[PDF]</a>
                <a href="https://cogact.github.io/">[Project]</a>
		<a href="https://github.com/microsoft/CogACT">[Code]</a>
                <br>
                <p>We propose a new advanced VLA architecture for robot manipulation, which leverages cognitive information extracted by powerful VLMs to guide action prediction of a specialized action module.</p>
            </td>
        </tr>	

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/moge.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>MoGe: Unlocking Accurate Monocular Geometry Estimation for Open-Domain Images with Optimal Training Supervision</papertitle>
                <br>
                Ruicheng Wang, Sicheng Xu, Cassie Dai, Jianfeng Xiang, <strong>Yu Deng</strong>, Xin Tong, Jiaolong Yang
                <br>
                <em>2025 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2025,
		    <strong><font color="#FF0000">Oral Presentation</font></strong>
                <br>
                <a href="https://arxiv.org/abs/2410.19115">[PDF]</a>
                <a href="https://wangrc.site/MoGePage/">[Project]</a>
		<a href="https://github.com/microsoft/moge">[Code]</a>
                <br>
                <p>We present MoGe, a powerful model for recovering 3D geometry from monocular open-domain images.</p>
            </td>
        </tr>	

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle" bgcolor="#ffffd0">
                <div class="one">
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/portrait4d-v2.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle" bgcolor="#ffffd0">
                <papertitle>Portrait4D-v2: Pseudo Multi-View Data Creates Better 4D Head Synthesizer</papertitle>
                <br>
                <strong>Yu Deng</strong>, Duomin Wang, Baoyuan Wang
                <br>
                <em>2024 European Conference on Computer Vision</em>, ECCV 2024,
                <br>
                <a href="https://arxiv.org/abs/2403.13570">[PDF]</a>
                <a href="https://yudeng.github.io/Portrait4D-v2/">[Project]</a>
		<a href="https://github.com/YuDeng/Portrait-4D">[Code]</a>
                <br>
                <p>We learn a lifelike 4D head synthesizer by creating pseudo multi-view videos from monocular ones as supervision.</p>
            </td>
        </tr>	

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle" bgcolor="#ffffd0">
                <div class="one">
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/portrait4d.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle" bgcolor="#ffffd0">
                <papertitle>Learning One-Shot 4D Head Avatar Synthesis using Synthetic Data</papertitle>
                <br>
                <strong>Yu Deng</strong>, Duomin Wang, Xiaohang Ren, Xingyu Chen, Baoyuan Wang
                <br>
                <em>2024 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2024,
                <br>
                <a href="https://arxiv.org/abs/2311.18729">[PDF]</a>
                <a href="https://yudeng.github.io/Portrait4D/">[Project]</a>
		<a href="https://github.com/YuDeng/Portrait-4D">[Code]</a>
                <br>
                <p>We propose a one-shot 4D head synthesis approach for high-fidelity 4D head avatar reconstruction while trained on large-scale synthetic data.</p>
            </td>
        </tr>	
		
        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle" bgcolor="#ffffd0">
                <div class="one">
		<a href="images/mimic3d.png"><img style="width:100%;max-width:100%; position: absolute;top: -5%" alt="Mimic3D" src="images/mimic3d.png" class="hoverZoomLink"></a>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle" bgcolor="#ffffd0">
                <papertitle>Mimic3D: Thriving 3D-Aware GANs via 3D-to-2D Imitation</papertitle>
                <br>
                Xingyu Chen*, <strong>Yu Deng*</strong>, Baoyuan Wang (* for equal contribution)
                <br>
                <em>2023 IEEE International Conference on Computer Vision</em>, ICCV 2023,
                <br>
                <a href="https://arxiv.org/abs/2303.09036">[PDF]</a>
                <a href="https://seanchenxy.github.io/Mimic3DWeb/">[Project]</a>
		<a href="https://github.com/SeanChenxy/Mimic3D">[Code]</a>
                <br>
                <p>We propose a novel approach that enables a 3D-aware GAN to generate images with both state-of-the-art photorealism and strict 3D consistency.</p>
            </td>
        </tr>		

	<tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/GRAMHD.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>GRAM-HD: 3D-Consistent Image Generation at High Resolution with Generative Radiance Manifolds</papertitle>
                <br>
                Jianfeng Xiang, Jiaolong Yang, <strong>Yu Deng</strong>, Xin Tong
                <br>
                <em>2023 IEEE International Conference on Computer Vision</em>, ICCV 2023,
                <br>
                <a href="https://arxiv.org/abs/2206.07255">[PDF]</a>
                <a href="https://jeffreyxiang.github.io/GRAM-HD/">[Project]</a>
		    <a href="images/GRAM-HD.txt">[BibTeX]</a>
                <br>
                <p>We propose GRAM-HD, a 3D-aware GAN that can generate photorealistic and 3D-consistent images at 1024x1024 resolution.</p>
            </td>
        </tr>
	
        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/graminverter.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Learning Detailed Radiance Manifolds for High-Fidelity and 3D-Consistent Portrait Synthesis from Monocular Image</papertitle>
                <br>
                <strong>Yu Deng</strong>, Baoyuan Wang, Heung-Yeung Shum
                <br>
                <em>2023 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2023,
                <br>
                <a href="https://arxiv.org/abs/2211.13901">[PDF]</a>
                <a href="https://yudeng.github.io/GRAMInverter/">[Project]</a>
		    <a href="images/graminverter.txt">[BibTeX]</a>
                <br>
                <p>We propose a learning-based approach for high-fidelity and 3D-consistent novel view synthesis of monocular portrait images.</p>
            </td>
        </tr>		

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/pdfgc.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis</papertitle>
                <br>
                Duomin Wang, <strong>Yu Deng</strong>, Zixin Yin, Heung-Yeung Shum, Baoyuan Wang
                <br>
                <em>2023 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2023,
                <br>
                <a href="https://arxiv.org/abs/2211.14506">[PDF]</a>
                <a href="https://dorniwang.github.io/PD-FGC/">[Project]</a>
                <br>
                <p>We propose a one-shot talking head synthesis approach with disentangled control over lip motion, eye gaze&blink, head pose, and emotional expression.</p>
            </td>
        </tr>		      
	      
		
        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/anifacegan.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>AniFaceGAN: Animatable 3D-Aware Face Image Generation for Video Avatars</papertitle>
                <br>
                Yue Wu, <strong>Yu Deng</strong>, Jiaolong Yang, Fangyun Wei, Qifeng Chen,  Xin Tong
                <br>
                <em>2022 Conference on Neural Information Processing Systems</em>, NeurIPS 2022,
		    <strong><font color="#FF8080">Spotlight</font></strong>
                <br>
                <a href="https://arxiv.org/abs/2210.06465">[PDF]</a>
                <a href="https://yuewuhkust.github.io/AniFaceGAN/">[Project]</a>
		    <a href="images/anifacegan.txt">[BibTeX]</a>
                <br>
                <p>We propose AniFaceGAN, an animatable 3D-aware GAN for multiview consistent face animation generation.</p>
            </td>
        </tr>		
	      
	<tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/gen_deform_nerf.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Generative Deformable Radiance Fields for Disentangled Image Synthesis of Topology-Varying Objects</papertitle>
                <br>
                Ziyu Wang, <strong>Yu Deng</strong>, Jiaolong Yang, Jingyi Yu, Xin Tong
                <br>
                <em>The 30th Pacific Graphics Conference</em>, PG 2022,
                <br>
                <a href="https://arxiv.org/abs/2209.04183">[PDF]</a>
		    <a href="https://ziyuwang98.github.io/GDRF/">[Project]</a>
		    <a href="https://github.com/ziyuwang98/GDRF/">[Code]</a>
		    <a href="images/GDRF.txt">[BibTeX]</a>
                <br>
                <p>We propose a generative model for synthesizing radiance fields of topology-varying objects with disentangled shape and appearance variations.</p>
            </td>
        </tr>

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle" bgcolor="#ffffd0">
                <div class="one">
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/gram.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle" bgcolor="#ffffd0">
                <papertitle>GRAM: Generative Radiance Manifolds for 3D-Aware Image Generation</papertitle>
                <br>
                <strong>Yu Deng</strong>, Jiaolong Yang, Jianfeng Xiang, Xin Tong
                <br>
                <em>2022 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2022,
		    <strong><font color="#FF0000">Oral Presentation</font></strong>
                <br>
                <a href="https://arxiv.org/abs/2112.08867">[PDF]</a>
                <a href="https://yudeng.github.io/GRAM/">[Project]</a>
		<a href="https://github.com/microsoft/GRAM">[Code]</a>
				<a href="images/gram.txt">[BibTeX]</a>
                <br>
                <p>We propose Generative Radiance Manifolds (GRAM), a method that can generate 3D-consistent images with explicit camera control, trained on only unstructured 2D images.</p>
            </td>
        </tr>

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                <img src='images/arxiv_difnet.png' style="width:100%;max-width:100%; position: absolute;top: -5%">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Deformed Implicit Field: Modeling 3D Shapes with Learned Dense Correspondence</papertitle>
                <br>
                <strong>Yu Deng</strong>, Jiaolong Yang, Xin Tong
                <br>
                <em>2021 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2021,
                <br>
                <a href="https://arxiv.org/abs/2011.13650">[PDF]</a>
                <a href="https://github.com/microsoft/DIF-Net">[Code]</a>
				<a href="images/dif_net.txt">[BibTeX]</a>
                <br>
                <p>We propose a novel Deformed Implicit Field (DIF) representation for modeling 3D shapes of a category and generating dense correspondences among shapes with structure variations.</p>
            </td>
        </tr>
          
        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle" bgcolor="#ffffd0">
                <div class="one" >
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/disco.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle" bgcolor="#ffffd0">
                <papertitle>Disentangled and Controllable Face Image Generation via 3D Imitative-Contrastive Learning</papertitle>
                <br>
                <strong>Yu Deng</strong>, Jiaolong Yang, Dong Chen, Fang Wen, Xin Tong
                <br>
                <em>2020 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2020, 
				<strong><font color="#FF0000">Oral Presentation</font></strong>
                <br>
                <a href="https://arxiv.org/abs/2004.11660">[PDF]</a>
                <a href="https://github.com/microsoft/DiscoFaceGAN">[Code]</a>
				<a href="images/discoface.txt">[BibTeX]</a>
                <br>
                <p>We propose DiscoFaceGAN, an approach for face image generation of virtual people with disentangled, precisely-controllable latent representations for identity, expression, pose, and illumination. </p>
            </td>
        </tr>

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one" >
			<a href="images/cvpr20_deep3dportrait.png"><img style="width:100%;max-width:100%; position: absolute;top: -5%" alt="Deep3DPortrait" src="images/cvpr20_deep3dportrait.png" class="hoverZoomLink"></a>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Deep 3D Portrait from a Single Image</papertitle>
                <br>
                Sicheng Xu, Jiaolong Yang, Dong Chen, Fang Wen, <strong>Yu Deng</strong>, Yunde Jia, Xin Tong
                <br>
                <em>2020 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2020, 
                <br>
                <a href="https://arxiv.org/abs/2004.11598">[PDF]</a>
                <a href="https://github.com/sicxu/Deep3dPortrait">[Code]</a>
				<a href="images/deep3dportrait.txt">[BibTeX]</a>
                <br>
                <p>We propose a learning-based approach for recovering the 3D geometry of human head from a single portrait image without any ground-truth 3D data.</p>
            </td>
        </tr>

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle" bgcolor="#ffffd0">
                <div class="one" >
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/accurate3d.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle" bgcolor="#ffffd0">
                <papertitle>Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set</papertitle>
                <br>
                <strong>Yu Deng</strong>, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde Jia, Xin Tong
                <br>
                <em>2019 IEEE Conference on Computer Vision and Pattern Recognition Workshop on AMFG</em>, CVPRW 2019, 
                <strong><font color="#FF0000">Best Paper Award</font></strong>
                <br>
                <a href="https://arxiv.org/abs/1903.08527">[PDF]</a>
                <a href="https://github.com/microsoft/Deep3DFaceReconstruction">[Code]</a>
				<a href="images/accurate3d.txt">[BibTeX]</a>
                <br>
                <p>We propose a novel deep 3D face reconstruction approach that leverages a robust hybrid loss function and performs multi-image face reconstruction by exploiting complementary information from different images for shape aggregation.</p>
            </td>
        </tr>

      </td>
    </tr>
  </table>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      <heading>Academic Services</heading>
    </td> 
  </tr>
</tbody>
</table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <td style="padding:20px;width:75%;vertical-align:middle">
      <strong>Conference Reviewer:</strong> CVPR, ICCV, SIGGRAPH, SIGGRAPH Asia
      <br>
      <strong>Journal Reviewer:</strong> TPAMI, TVCG

  </td>
</tbody>
</table>
<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="width:0%;vertical-align:middle">
        </td>
        <td style="width:100%;vertical-align:middle">
        <hr style="margin-top:0px">
            <p><font color="#999999">The website template was adapted from <a href="https://jonbarron.info/">Jon Barron</a>.</font></p>
        </td>
    </tr>
  </tbody>
</table>
</body>

</html>
